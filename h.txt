B1
#measure genomic nucleotide composition.
#proportion genome Gaunine and cytosine
def calculate_gc_content(sequence):
    gc_count = sequence.count('G') + sequence.count('C')
    total_bases = len(sequence)
    gc_content = (gc_count / total_bases) * 100
    return gc_content

#the process of discovering patterns having biological function within collections of sequences
def find_motifs(sequence, motif_length):
    motifs = []
    for i in range(len(sequence) - motif_length + 1):
        motif = sequence[i:i+motif_length]
        motifs.append(motif)
    return motifs
#Exons
#sequence of three nucleotides (a trinucleotide) that forms a unit of genomic information encoding a particular amino acid
def identify_coding_regions(sequence, min_coding_length):
    coding_regions = []
    in_coding_region = False
    current_coding_start = 0

    for i in range(len(sequence) - 2):
        codon = sequence[i:i+3]
        #ATG start codon protein synthesis codes for amino acid
        if codon == 'ATG' and not in_coding_region:
            in_coding_region = True
            current_coding_start = i
        #TAA TAG TGA stop codon end of translation does not code for amino acid
        elif codon in ['TAA', 'TAG', 'TGA'] and in_coding_region:
            in_coding_region = False
            coding_length = i + 3 - current_coding_start
            if coding_length >= min_coding_length:
                coding_regions.append((current_coding_start, i + 3))

    return coding_regions
#
Example DNA sequence
dna_sequence = "ATGCGTACGTAGCTAGCTAGTAAATGTAGCTAGCTAGCTGATATA"

# Calculate GC content
gc_content = calculate_gc_content(dna_sequence)
print(f"GC Content: {gc_content:.2f}%")

# Find motifs
motif_length = 4
motifs = find_motifs(dna_sequence, motif_length)
print("Motifs:", motifs)

# Identify coding regions
min_coding_length = 6
coding_regions = identify_coding_regions(dna_sequence, min_coding_length)
print("Predicted Coding Regions:", coding_regions)

B2
import pandas as pd
import numpy as np
import scanpy as sc

df = sc.read_csv(
df.X.shape
df.var['mt'] = df.var.index.str.startswith('MT-')

ribo_url = r"C:\Users\Gaju\Desktop\Aditee\AJ\Bioinfo\Bioinfo2\KEGG_RIBOSOME.v2023.2.Hs.txt"
ribo_genes = pd.read_table(ribo_url, skiprows=2, header = None)

df.var['ribo'] = df.var_names.isin(ribo_genes[0].values)

sc.pp.calculate_qc_metrics(df, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)

sc.pp.filter_genes(df, min_cells=3)

df.obs.columns

sc.pl.violin(df, ['n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt',
       'total_counts_ribo', 'pct_counts_ribo'], multi_panel=True, jitter=0.4)

upper_lim = np.quantile(df.obs.n_genes_by_counts, 0.98)
upper_lim

df = df[df.obs.n_genes_by_counts < upper_lim]
df = df[df.obs.pct_counts_mt < 20]
df = df[df.obs.pct_counts_ribo < 2]


sc.pp.normalize_total(df, target_sum=1e4)
sc.pp.log1p(df)

sc.pp.highly_variable_genes(df, n_top_genes=2000)

sc.pl.highly_variable_genes(df)

adata = sc.read_h5ad(r'combined.h5ad')
adata

sc.pp.filter_genes(adata, min_cells=100)

adata.layers["counts"] = adata.X.copy()

!pip install leidenalg

sc.tl.umap(adata)
sc.tl.leiden(adata, resolution=0.5)

adata.write(r'integrated.h5ad')

sc.tl.leiden(adata, resolution=1)

sc.tl.rank_genes_groups(adata, 'leiden')

markers = sc.get.rank_genes_groups_df(adata,None)

markers = markers[ (markers.pvals_adj < 0.05) & (markers.logfoldchanges > 0.5) ]

markers

sc.pl.umap(adata, color='leiden')

B5

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, auc

df5 = pd.read_csv(r"C:\Users\Gaju\Desktop\Aditee\New\BE AIDS DATASETS\Bioinformatics\Assignment 5\human_data1.csv")
df5.head()


X = df5['sequence']
y = df5['class']

le = LabelEncoder()
X = le.fit_transform(X)

X = X.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)
rfc_acc = rfc.score(X_test, y_test)

svc = SVC()
svc.fit(X_train, y_train)
svc_acc = svc.score(X_test, y_test)

#creating a dataframe for plotting
accuracy_data = {'Model': ['RFC', 'SVC'], 'Accuracy': [rfc_acc, svc_acc]}
accuracy_df = pd.DataFrame(accuracy_data)

plt.figure(figsize=(8, 5))
sns.barplot(x='Model', y='Accuracy', data=accuracy_df)
plt.title('Accuracy Comparison between RFC and SVC')
plt.ylim(0, 1)
plt.show()

y_pred_rfc = rfc.predict(X_test)
y_pred_svc = svc.predict(X_test)
cm_rfc = confusion_matrix(y_test, y_pred_rfc)
cm_svc = confusion_matrix(y_test, y_pred_svc)

disp_rfc = ConfusionMatrixDisplay(confusion_matrix=cm_rfc)
disp_rfc.plot(cmap='Blues')
plt.title('RFC Confusion Matrix')
plt.show()

disp_svc = ConfusionMatrixDisplay(confusion_matrix=cm_svc)
disp_svc.plot(cmap='Blues')
plt.title('SVC Confusion Matrix')
plt.show()

I1

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')


def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text)
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)


def stem_text(text):
    stemmer = PorterStemmer()
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)


def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = nltk.word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(lemmatized_words)


import nltk
nltk.download('omw-1.4')

def main():
    # Sample text
    text = "In a lively city, skyscrapers cast dynamic shadows, enticing exploration of local eateries. Tranquil parks offer an escape from urban life, blending modernity with history."

    # Remove stop words
    text_without_stopwords = remove_stopwords(text)
    print("Text after stop word removal:")
    print(text_without_stopwords)
    print()

    # Stemming
    stemmed_text = stem_text(text)
    print("Text after stemming:")
    print(stemmed_text)
    print()

    # Lemmatization (using NLTK)
    lemmatized_text = lemmatize_text(text)
    print("Text after lemmatization:")
    print(lemmatized_text)

if __name__ == "__main__":
    main()

I2

IR Assig 2

import string

document1 = "The dog barked at the sleeping cat"
document2 = "The lazy cat was sleeping under the shed."

def remove_p(doc):
  doc = doc.translate(str.maketrans("", "", string.punctuation))
  return doc

document1 = remove_p(document1)
document2 = remove_p(document2)

tokens1 = document1.lower().split()
tokens2 = document2.lower().split()

terms = list(set(tokens1 + tokens2))

inverted_index = {}

for term in terms:
	documents = []
	if term in tokens1:
		documents.append(["Document 1", f"Index loc: {tokens1.index(term)}"])
	if term in tokens2:
		documents.append(["Document 2", f"Index loc: {tokens2.index(term)}"])
	inverted_index[term] = documents

key = input('Enter key to be searched: ').lower()

if key in inverted_index:
	print(key, "-", inverted_index[key])
else:
  print("Not present.")

print(terms)

print(inverted_index)

I3

IR Assig 3 

Update

!pip install numpy==1.23 --user

!pip install typing-extensions --user

!pip install sympy --user

!pip install Cython --user

!pip install torch --user

#Kernel - restart and run all

!pip install pgmpy --user

import pandas as pd
from pgmpy.estimators import MaximumLikelihoodEstimator     #extimate maximum likelihood parameters
from pgmpy.models import BayesianNetwork                      #imports bayesian model
from pgmpy.inference import VariableElimination             #computer probability of variables

heartDisease = pd.read_csv(r"C:\Users\Gaju\Desktop\Aditee\New\BE AIDS DATASETS\Information Retrieval\Assignment 3\heart.csv")
heartDisease.isna().sum()       #remove null values

heartDisease.head()

#define bayesian model
m = BayesianNetwork( [('age', 'target'), ('sex','target'), ('chol', 'target'), ('thalach','target'), ('target','exang'), ('target', 'restecg')] )
m.fit(heartDisease, estimator=MaximumLikelihoodEstimator)       #helps find relations between parameters
infer = VariableElimination(m)                                  #determines relationships

q1 = infer.query(variables=['target', 'exang'], evidence={'restecg': 1})           #abt target when exang is given
print("The probability of having a certain target when exang is given is : \n")
print(q1)

I4

IR Assig 4

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer         #differentiates imp words in document
from sklearn.naive_bayes import MultinomialNB                       #makes predictions 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# Load the dataset
file_path = r"C:\Users\Gaju\Desktop\Aditee\New\BE AIDS DATASETS\Information Retrieval\Assignment 4\spam.csv"
data = pd.read_csv(file_path)


data.columns

data.head(10)

# Data Preprocessing
data.dropna(inplace=True)                                   #removes missing values
data['Category'] = data['Category'].map({'spam': 1, 'ham': 0})    #Converting labels to numerical values

# Feature Extraction
X = data['Message']
y = data['Category']
vectorizer = TfidfVectorizer(max_features=5000)     #assigns num values to upyo 5000 imp words
X = vectorizer.fit_transform(X)                     


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training
model = MultinomialNB()             #deals with multiple categories and outcomes
model.fit(X_train, y_train)


# Model Evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)       #define accuracy based on trained data


print("Accuracy:", accuracy)


print("Classification Report:")
print(classification_report(y_test, y_pred))


print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


I5

IR assig 5

import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.cluster import AgglomerativeClustering     #unsupervised ml algo, grp similar dp into clusters
from scipy.cluster.hierarchy import linkage             #group similar datapts into cluster

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data

# Apply Agglomerative Hierarchical Clustering
n_clusters = int(input("Enter no of clusters: "))  # Number of clusters to form
agglomerative = AgglomerativeClustering(n_clusters=n_clusters)
agglomerative.fit(X)
labels = agglomerative.labels_              #retrives cluster labels from model

# Calculate linkage matrix for dendrogram
Z = linkage(X, method='ward')               #calls linkage fn 

#Plot each cluster with a different color
for cluster_num in range(n_clusters):
    cluster_points = X[labels == cluster_num]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_num}')

plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.title('Scatter plot of Agglomerative Clustering')
plt.legend()
plt.show()

# Print cluster assignments
print("Cluster labels:", labels)













